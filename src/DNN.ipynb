{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_Preparation.fish_survival_data_preparation import create_fish_pipeline, prepare_fish_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and Preprocessing the Data here. Basically just getting the data prepared for \n",
    "# DNN model.\n",
    "X_train, X_dev, X_test, y_train_orig, y_dev_orig, y_test_orig = prepare_fish_data(ratios=(0.1, 0.1))\n",
    "\n",
    "pipeline = create_fish_pipeline()\n",
    "X_train_scaled = pipeline.fit_transform(X_train)\n",
    "X_dev_scaled = pipeline.transform(X_dev)\n",
    "X_test_scaled = pipeline.transform(X_test)\n",
    "\n",
    "# Normalizing the target value here as the variance is very low in y.\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train_orig.to_numpy().reshape(-1, 1)).flatten()\n",
    "y_dev_scaled = target_scaler.transform(y_dev_orig.to_numpy().reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test_orig.to_numpy().reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried a model with more layers but got not so good reusults (we can alomst say bad) so \n",
    "# Buildt a new DNN Model function with less layers.\n",
    "\n",
    "def build_dnn_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='relu') \n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_dnn_model(X_train_scaled.shape[1])\n",
    "model.compile(optimizer=Adam(), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step - loss: 0.5425 - val_loss: 6.2301\n",
      "Epoch 2/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - loss: 1.1104 - val_loss: 6.2301\n",
      "Epoch 3/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 1.0132 - val_loss: 6.2301\n",
      "Epoch 4/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.7913 - val_loss: 6.2301\n",
      "Epoch 5/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 0.6001 - val_loss: 6.2301\n",
      "Epoch 6/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.9743 - val_loss: 6.2301\n",
      "Epoch 7/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - loss: 0.8245 - val_loss: 6.2301\n",
      "Epoch 8/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 1.1786 - val_loss: 6.2301\n",
      "Epoch 9/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - loss: 0.9393 - val_loss: 6.2301\n",
      "Epoch 10/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450us/step - loss: 0.7220 - val_loss: 6.2301\n",
      "Epoch 11/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491us/step - loss: 0.7362 - val_loss: 6.2301\n",
      "Epoch 12/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - loss: 0.5050 - val_loss: 6.2301\n",
      "Epoch 13/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - loss: 0.6670 - val_loss: 6.2301\n",
      "Epoch 14/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step - loss: 0.9680 - val_loss: 6.2301\n",
      "Epoch 15/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - loss: 0.9682 - val_loss: 6.2301\n",
      "Epoch 16/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 1.0853 - val_loss: 6.2301\n",
      "Epoch 17/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - loss: 0.4235 - val_loss: 6.2301\n",
      "Epoch 18/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - loss: 0.6739 - val_loss: 6.2301\n",
      "Epoch 19/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 1.3485 - val_loss: 6.2301\n",
      "Epoch 20/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - loss: 1.3425 - val_loss: 6.2301\n",
      "Epoch 21/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step - loss: 1.3951 - val_loss: 6.2301\n",
      "Epoch 22/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - loss: 0.7635 - val_loss: 6.2301\n",
      "Epoch 23/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step - loss: 0.5313 - val_loss: 6.2301\n",
      "Epoch 24/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 1.0439 - val_loss: 6.2301\n",
      "Epoch 25/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447us/step - loss: 1.1003 - val_loss: 6.2301\n",
      "Epoch 26/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432us/step - loss: 0.6932 - val_loss: 6.2301\n",
      "Epoch 27/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447us/step - loss: 0.5490 - val_loss: 6.2301\n",
      "Epoch 28/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - loss: 1.0866 - val_loss: 6.2301\n",
      "Epoch 29/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - loss: 0.5611 - val_loss: 6.2301\n",
      "Epoch 30/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 0.7039 - val_loss: 6.2301\n",
      "Epoch 31/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - loss: 0.6477 - val_loss: 6.2301\n",
      "Epoch 32/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step - loss: 0.6614 - val_loss: 6.2301\n",
      "Epoch 33/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438us/step - loss: 0.9614 - val_loss: 6.2301\n",
      "Epoch 34/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - loss: 1.1171 - val_loss: 6.2301\n",
      "Epoch 35/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450us/step - loss: 1.2313 - val_loss: 6.2301\n",
      "Epoch 36/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 1.3257 - val_loss: 6.2301\n",
      "Epoch 37/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439us/step - loss: 0.4916 - val_loss: 6.2301\n",
      "Epoch 38/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - loss: 0.5868 - val_loss: 6.2301\n",
      "Epoch 39/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 445us/step - loss: 0.5666 - val_loss: 6.2301\n",
      "Epoch 40/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step - loss: 1.0915 - val_loss: 6.2301\n",
      "Epoch 41/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.7325 - val_loss: 6.2301\n",
      "Epoch 42/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.9691 - val_loss: 6.2301\n",
      "Epoch 43/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 1.3943 - val_loss: 6.2301\n",
      "Epoch 44/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.6945 - val_loss: 6.2301\n",
      "Epoch 45/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - loss: 0.6986 - val_loss: 6.2301\n",
      "Epoch 46/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - loss: 1.3257 - val_loss: 6.2301\n",
      "Epoch 47/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - loss: 1.1966 - val_loss: 6.2301\n",
      "Epoch 48/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step - loss: 1.0472 - val_loss: 6.2301\n",
      "Epoch 49/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - loss: 0.7360 - val_loss: 6.2301\n",
      "Epoch 50/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - loss: 0.8446 - val_loss: 6.2301\n",
      "Epoch 51/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - loss: 1.0688 - val_loss: 6.2301\n",
      "Epoch 52/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - loss: 2.0318 - val_loss: 6.2301\n",
      "Epoch 53/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.6671 - val_loss: 6.2301\n",
      "Epoch 54/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - loss: 0.9584 - val_loss: 6.2301\n",
      "Epoch 55/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - loss: 0.7547 - val_loss: 6.2301\n",
      "Epoch 56/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450us/step - loss: 0.3634 - val_loss: 6.2301\n",
      "Epoch 57/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 1.0487 - val_loss: 6.2301\n",
      "Epoch 58/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - loss: 0.9233 - val_loss: 6.2301\n",
      "Epoch 59/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step - loss: 0.6350 - val_loss: 6.2301\n",
      "Epoch 60/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - loss: 0.8562 - val_loss: 6.2301\n",
      "Epoch 61/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450us/step - loss: 0.8519 - val_loss: 6.2301\n",
      "Epoch 62/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - loss: 0.9311 - val_loss: 6.2301\n",
      "Epoch 63/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447us/step - loss: 0.9648 - val_loss: 6.2301\n",
      "Epoch 64/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439us/step - loss: 0.9025 - val_loss: 6.2301\n",
      "Epoch 65/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - loss: 1.4895 - val_loss: 6.2301\n",
      "Epoch 66/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - loss: 1.4814 - val_loss: 6.2301\n",
      "Epoch 67/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - loss: 0.6147 - val_loss: 6.2301\n",
      "Epoch 68/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - loss: 0.7292 - val_loss: 6.2301\n",
      "Epoch 69/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - loss: 0.7756 - val_loss: 6.2301\n",
      "Epoch 70/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - loss: 2.4754 - val_loss: 6.2301\n",
      "Epoch 71/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450us/step - loss: 1.4720 - val_loss: 6.2301\n",
      "Epoch 72/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 2.3249 - val_loss: 6.2301\n",
      "Epoch 73/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - loss: 0.5599 - val_loss: 6.2301\n",
      "Epoch 74/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.9961 - val_loss: 6.2301\n",
      "Epoch 75/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - loss: 0.4936 - val_loss: 6.2301\n",
      "Epoch 76/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - loss: 0.7436 - val_loss: 6.2301\n",
      "Epoch 77/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477us/step - loss: 0.5514 - val_loss: 6.2301\n",
      "Epoch 78/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - loss: 3.4866 - val_loss: 6.2301\n",
      "Epoch 79/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.8514 - val_loss: 6.2301\n",
      "Epoch 80/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450us/step - loss: 1.1172 - val_loss: 6.2301\n",
      "Epoch 81/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step - loss: 0.5589 - val_loss: 6.2301\n",
      "Epoch 82/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - loss: 0.4656 - val_loss: 6.2301\n",
      "Epoch 83/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - loss: 0.8447 - val_loss: 6.2301\n",
      "Epoch 84/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - loss: 2.5548 - val_loss: 6.2301\n",
      "Epoch 85/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - loss: 0.8211 - val_loss: 6.2301\n",
      "Epoch 86/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 1.2900 - val_loss: 6.2301\n",
      "Epoch 87/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step - loss: 0.7919 - val_loss: 6.2301\n",
      "Epoch 88/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - loss: 0.6220 - val_loss: 6.2301\n",
      "Epoch 89/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477us/step - loss: 1.4742 - val_loss: 6.2301\n",
      "Epoch 90/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - loss: 0.7311 - val_loss: 6.2301\n",
      "Epoch 91/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - loss: 0.7660 - val_loss: 6.2301\n",
      "Epoch 92/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - loss: 0.8091 - val_loss: 6.2301\n",
      "Epoch 93/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - loss: 1.0558 - val_loss: 6.2301\n",
      "Epoch 94/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - loss: 1.0444 - val_loss: 6.2301\n",
      "Epoch 95/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6816 - val_loss: 6.2301\n",
      "Epoch 96/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - loss: 1.1404 - val_loss: 6.2301\n",
      "Epoch 97/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.5402 - val_loss: 6.2301\n",
      "Epoch 98/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step - loss: 0.5245 - val_loss: 6.2301\n",
      "Epoch 99/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 1.8265 - val_loss: 6.2301\n",
      "Epoch 100/100\n",
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - loss: 0.5789 - val_loss: 6.2301\n"
     ]
    }
   ],
   "source": [
    "# Training the Model here\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_dev_scaled, y_dev_scaled),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=100, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created an evaluation function to track the results of the DNN model\n",
    "# on all the kinds of data.\n",
    "\n",
    "def evaluate_dnn(model, X, y_scaled, y_original, label):\n",
    "    y_pred_scaled = model.predict(X).flatten()\n",
    "    y_pred = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    rmse = np.sqrt(mean_squared_error(y_original, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_original, y_pred)\n",
    "    r2 = r2_score(y_original, y_pred)\n",
    "    print(f\"\\n {label} Set Performance:\")\n",
    "    print(f\"Mean y: {np.mean(y_original):.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    return rmse, mape, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m649/649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step\n",
      "\n",
      "📊 Train Set Performance:\n",
      "Mean y: 99.9757\n",
      "RMSE: 0.2598\n",
      "MAPE: 0.0004\n",
      "R²: -0.0000\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275us/step\n",
      "\n",
      "📊 Dev Set Performance:\n",
      "Mean y: 99.9646\n",
      "RMSE: 0.6484\n",
      "MAPE: 0.0006\n",
      "R²: -0.0003\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275us/step\n",
      "\n",
      "📊 Test Set Performance:\n",
      "Mean y: 99.9775\n",
      "RMSE: 0.1815\n",
      "MAPE: 0.0004\n",
      "R²: -0.0001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.18153383910176624, 0.00038275855204272286, -0.00010259928762179626)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By using the evaluating function above this is done.\n",
    "\n",
    "evaluate_dnn(model, X_train_scaled, y_train_scaled, y_train_orig, \"Train\")\n",
    "evaluate_dnn(model, X_dev_scaled, y_dev_scaled, y_dev_orig, \"Dev\")\n",
    "evaluate_dnn(model, X_test_scaled, y_test_scaled, y_test_orig, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
