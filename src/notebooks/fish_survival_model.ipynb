{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from joblib import dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from data_preparation.fish_survival_data_preparation import create_fish_pipeline, prepare_fish_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_xgb(X_train, y_train, X_dev, y_dev):\n",
    "    print(\"Evaluating XGBoost Regressor...\")\n",
    "\n",
    "    # Define the hyperparameter grid search to try combinations of these hyperparameters.\n",
    "    param_grid = {\n",
    "        'algo__n_estimators': [1000],\n",
    "        'algo__max_depth': [2, 3, 4],\n",
    "        'algo__learning_rate': [0.01, 0.05, 0.1], # smaller learning rate is possibly better as training consisitency increasees.\n",
    "        'algo__subsample': [0.8, 1.0],\n",
    "\n",
    "    }\n",
    "\n",
    "    # This here uses the pipeline to handle missing values, scaling, encoding, etc for teh dataset.\n",
    "    pipeline = create_fish_pipeline()\n",
    "\n",
    "    # This combines the preprocessing and XGBoost model into one clean pipeline.\n",
    "    pipeline_with_algo = Pipeline(steps=[\n",
    "        ('preprocessor', pipeline),\n",
    "        ('algo', XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline_with_algo, param_grid,\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        scoring='neg_mean_squared_error',  \n",
    "        verbose=1  # Show progress in terminal\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # This shows us our best model based on cross-validation RÂ² score.\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "\n",
    "    # ðŸ“Š FEATURE IMPORTANCE SECTION\n",
    "    try:\n",
    "        model = best_estimator.named_steps[\"algo\"]\n",
    "        preprocessor = best_estimator.named_steps[\"preprocessor\"]\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        importances = model.feature_importances_\n",
    "\n",
    "        feature_df = pd.DataFrame({\n",
    "            \"Feature\": feature_names,\n",
    "            \"Importance\": importances\n",
    "        }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_df.head(10))\n",
    "    except Exception as e:\n",
    "        print(\"Could not extract feature importances:\", e)\n",
    "\n",
    "    # We are making predicitons on the dev set here\n",
    "    y_pred = best_estimator.predict(X_dev)\n",
    "\n",
    "    # Here we are calculating the following values\n",
    "    # Calculate evaluation metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_dev, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_dev, y_pred)\n",
    "    r2 = r2_score(y_dev, y_pred)\n",
    "\n",
    "    # Shows you the best performance from the training phase and the hyperparameters that gave it.\n",
    "    print(\"Grid searching is done!\")\n",
    "    print(\"Best score (neg MSE):\", grid_search.best_score_)\n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    return best_estimator, rmse, mape, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a reusable function to evaluate metrics\n",
    "def evaluate_metrics(y_true, y_pred, label):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mean_target = np.mean(y_true)\n",
    "    print(f\"\\nðŸ“Š {label} Set Performance:\")\n",
    "    print(f\"Mean of y_{label.lower()}: {mean_target:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}\")\n",
    "    print(f\"RÂ²: {r2:.4f}\")\n",
    "    return rmse, mape, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Evaluating model for: fish survival model)\n",
      "Evaluating XGBoost Regressor...\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                          Feature  Importance\n",
      "1               num__Max air temp    0.151554\n",
      "17  transparency__PM Transparency    0.138291\n",
      "0            num__Spring Temp (F)    0.103562\n",
      "5                     num__# fish    0.081378\n",
      "12          num__Dec Rain (Lag 1)    0.074135\n",
      "16  transparency__AM Transparency    0.069202\n",
      "6         num__Spring_Temp x Rain    0.051962\n",
      "8           num__Dec Rain (Lag 3)    0.047732\n",
      "19             cat__Season_Spring    0.044108\n",
      "15     num__Calmar Rain 7-day avg    0.034960\n",
      "Grid searching is done!\n",
      "Best score (neg MSE): -0.06556099133870939\n",
      "Best hyperparameters:\n",
      "{'algo__learning_rate': 0.1, 'algo__max_depth': 2, 'algo__n_estimators': 1000, 'algo__subsample': 1.0}\n",
      "âœ… Data Split Shapes:\n",
      "  X_train: (21400, 19)\n",
      "  X_dev: (2674, 19)\n",
      "  X_test: (2674, 19)\n",
      "  y_train: (21400,)\n",
      "  y_dev: (2674,)\n",
      "  y_test: (2674,)\n",
      "\n",
      "ðŸ“Š Train Set Performance:\n",
      "Mean of y_train: 99.9731\n",
      "RMSE: 0.2302\n",
      "MAPE: 0.0004\n",
      "RÂ²: 0.2343\n",
      "\n",
      "ðŸ“Š Dev Set Performance:\n",
      "Mean of y_dev: 99.9635\n",
      "RMSE: 0.6407\n",
      "MAPE: 0.0006\n",
      "RÂ²: -0.0010\n",
      "\n",
      "ðŸ“Š Test Set Performance:\n",
      "Mean of y_test: 99.9745\n",
      "RMSE: 0.2159\n",
      "MAPE: 0.0005\n",
      "RÂ²: -0.2651\n",
      "âœ… Model saved as: models/fish_survial_model.joblib\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"\\nðŸš€ Evaluating model for: fish survival model)\")\n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test = prepare_fish_data(ratios=((1/10), (1/10)))\n",
    "\n",
    "    best_model, _, _, _ = evaluate_xgb(X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "    print(\"âœ… Data Split Shapes:\")\n",
    "    print(\"  X_train:\", X_train.shape)\n",
    "    print(\"  X_dev:\", X_dev.shape)\n",
    "    print(\"  X_test:\", X_test.shape)\n",
    "    print(\"  y_train:\", y_train.shape)\n",
    "    print(\"  y_dev:\", y_dev.shape)\n",
    "    print(\"  y_test:\", y_test.shape)\n",
    "\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_dev_pred = best_model.predict(X_dev)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    evaluate_metrics(y_train, y_train_pred, \"Train\")\n",
    "    evaluate_metrics(y_dev, y_dev_pred, \"Dev\")\n",
    "    evaluate_metrics(y_test, y_test_pred, \"Test\")\n",
    "\n",
    "    # âœ… Save model\n",
    "    dump(best_model, \"../models/fish_survial_model.joblib\")\n",
    "    dump(best_model, \"../../app/models/fish_survial_model.joblib\")\n",
    "\n",
    "    print(\"âœ… Model saved as: models/fish_survial_model.joblib\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num__Spring Temp (F)' 'num__Max air temp' 'num__Min air temp'\n",
      " 'num__Dec Rain' 'num__Calmar Rain' 'num__# fish'\n",
      " 'num__Spring_Temp x Rain' 'num__Max Air Temp x Rain'\n",
      " 'num__Dec Rain (Lag 3)' 'num__Calmar Rain (Lag 3)'\n",
      " 'num__Dec Rain (Lag 2)' 'num__Calmar Rain (Lag 2)'\n",
      " 'num__Dec Rain (Lag 1)' 'num__Calmar Rain (Lag 1)'\n",
      " 'num__Dec Rain 7-day avg' 'num__Calmar Rain 7-day avg'\n",
      " 'transparency__AM Transparency' 'transparency__PM Transparency'\n",
      " 'cat__Season_Fall' 'cat__Season_Spring' 'cat__Season_Summer'\n",
      " 'cat__Season_Winter']\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "model = load(\"../models/fish_survial_model.joblib\")\n",
    "print(model.named_steps[\"preprocessor\"].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
